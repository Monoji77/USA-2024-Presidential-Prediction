---
title: "USA pollster"
subtitle: "CHANGE SUBTITLE"
author: 
  - Chris Yong Hong Sen
thanks: "Code and data are available at: [https://github.com/Monoji77/USA-pollster](https://github.com/Monoji77/USA-pollster)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(knitr)
data <- read_csv("../data/02-analysis_data/analysis_data.csv")
```


# Introduction

Overview paragraph

Estimand paragraph

Results paragraph

Why it matters paragraph

Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....






# Data {#sec-data}

## Overview

We use the statistical programming language R [@citeR].... Our data [@shelter].... Following @tellingstories, we consider...

Overview text

## Measurement
	
Some paragraphs about how we go from a phenomena in the world to an entry in the dataset.

## Predictor Variables

Add graphs, tables and text. Use sub-sub-headings for each outcome variable or update the subheading to be singular.



Some of our data is of penguins (@fig-bills), from @palmerpenguins.

```{r}
#| label: fig-bills
#| fig-cap: Bills of penguins
#| echo: false
data |>
  select(poll_id, pollster_id, candidate_name, winner)
ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"),
                    guide = "none") +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

Talk more about it.

And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)

```{r}
#| label: fig-planes
#| fig-cap: Relationship between wing length and width
#| echo: false
#| warning: false
#| message: false

analysis_data <- read_csv(here::here("data/02-analysis_data/analysis_data.csv"))
```

```{r}
raw_pollster_data <- read_csv('../data/01-raw_data/raw_pollster_data.csv')

raw_pollster_data |>
  colnames()

raw_pollster_data |>
  select(party, candidate_name) |>
  unique() |>
  arrange(party, candidate_name)

raw_pollster_data |>
  select(party, candidate_name) |>
  unique() |>
  filter(grepl('Donald', candidate_name))

# remove trailing time stamp
raw_pollster_data$created_at <- sapply(strsplit(raw_pollster_data$created_at, '\\s+'), '[', 1)

time_var <- c('start_date', 'end_date', 'created_at')

# get day, month, year in character data type
raw_pollster_data$month <- sapply(strsplit(raw_pollster_data$created_at, '/'), '[', 1)
raw_pollster_data$day <- sapply(strsplit(raw_pollster_data$created_at, '/'), '[', 2)
raw_pollster_data$year <- sapply(strsplit(raw_pollster_data$created_at, '/'), '[', 3) 

# observe how year is not cleaned
unique(raw_pollster_data$year)

# correct different formats for year
raw_pollster_data$year <- case_when(
  raw_pollster_data$year %in% c('24', '2024') ~ 2024,
  raw_pollster_data$year %in% c('23', '2023') ~ 2023,
  raw_pollster_data$year %in% c('22', '2022') ~ 2022,
  raw_pollster_data$year %in% c('21', '2021') ~ 2021,
  )

# check that year is cleaned
unique(raw_pollster_data$year)

raw_pollster_data <- raw_pollster_data |>
  mutate(day = ifelse(day < '10', paste0('0', day), day),
         month = ifelse(month < '10', paste0('0', month), month),
         created_at = as.Date(paste(year, month, day, sep='-'))) 

# get day, month, year in character data type
raw_pollster_data$month <- sapply(strsplit(raw_pollster_data$start_date, '/'), '[', 1)
raw_pollster_data$day <- sapply(strsplit(raw_pollster_data$start_date, '/'), '[', 2)
raw_pollster_data$year <- sapply(strsplit(raw_pollster_data$start_date, '/'), '[', 3) 

# observe how year is not cleaned
unique(raw_pollster_data$year)

# correct different formats for year
raw_pollster_data$year <- case_when(
  raw_pollster_data$year %in% c('24', '2024') ~ 2024,
  raw_pollster_data$year %in% c('23', '2023') ~ 2023,
  raw_pollster_data$year %in% c('22', '2022') ~ 2022,
  raw_pollster_data$year %in% c('21', '2021') ~ 2021,
  )

# check that year is cleaned
unique(raw_pollster_data$year)

raw_pollster_data <- raw_pollster_data |>
  mutate(day = ifelse(day < '10', paste0('0', day), day),
         month = ifelse(month < '10', paste0('0', month), month),
         start_date = as.Date(paste(year, month, day, sep='-'))) 

# get day, month, year in character data type
raw_pollster_data$month <- sapply(strsplit(raw_pollster_data$end_date, '/'), '[', 1)
raw_pollster_data$day <- sapply(strsplit(raw_pollster_data$end_date, '/'), '[', 2)
raw_pollster_data$year <- sapply(strsplit(raw_pollster_data$end_date, '/'), '[', 3) 

# observe how year is not cleaned
unique(raw_pollster_data$year)

# correct different formats for year
raw_pollster_data$year <- case_when(
  raw_pollster_data$year %in% c('24', '2024') ~ 2024,
  raw_pollster_data$year %in% c('23', '2023') ~ 2023,
  raw_pollster_data$year %in% c('22', '2022') ~ 2022,
  raw_pollster_data$year %in% c('21', '2021') ~ 2021,
  )

# check that year is cleaned
unique(raw_pollster_data$year)

raw_pollster_data <- raw_pollster_data |>
  mutate(day = ifelse(day < '10', paste0('0', day), day),
         month = ifelse(month < '10', paste0('0', month), month),
         end_date = as.Date(paste(year, month, day, sep='-'))) 

raw_pollster_data |>
  select(start_date, end_date, created_at) |>
  summarise(min(start_date))

sum(is.na(raw_pollster_data$partisan))

raw_pollster_data |>
  filter(!is.na(state))
  
raw_pollster_data |>
  select(office_type) |>
  unique()

raw_pollster_data |>
  summarise(min(pollscore, na.rm=T), max(pollscore, na.rm=T))

raw_pollster_data |>
  summarise(min(transparency_score, na.rm=T), max(transparency_score, na.rm=T))

raw_pollster_data |>
  summarise(min(numeric_grade, na.rm=T), max(numeric_grade, na.rm=T))

raw_pollster_data |>
  summarise(min(start_date, na.rm=T), max(start_date, na.rm=T),
            min(end_date, na.rm=T), max(end_date, na.rm=T))
```

Talk way more about it. 

## Predictor variables

Add graphs, tables and text.

Use sub-sub-headings for each outcome variable and feel free to combine a few into one if they go together naturally.








# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```




# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Patriot Polling: Wisconsin Presidential Analysis Data Collection 

## Methodology: Overview {#sec-patriot_polling_overview}
From 12 to 14 October 2024, Patriot Polling has conducted phone surveys in the state of Wisconsin of the United States of America, which borders Minnesota, Iowa, Michegan, and Illinois. Their target population are voters in Wisconsin. For sample of landlines which are characterised by households, an equal number of phone numbers are randomly selected across every landline block in Wisconsin using Random Digit Dialing (RDD). As for the sample of mobile numbers, which are characterised by an individual with access to a digital mobile device, the sample was purchased from a consumer contact number database. For this poll, a total of 803 respondents has completed the survey either from contact through the landline or their personal contact number. [@patriotPollingMethodology]

@patriotPollingMethodology conducted polling in such a way where a randomly contacted respondent would hear pre-recorded voice messages and users are able to interact with the automated phone system. This system is called Interactive voice response (IVR) which can handle outbound calls more systematically since the same questions would be asked sequentially in the survey. There were 2 questions provided by Patriot Polling that were asked to respondents. Both questions begin with 'How will you vote for president?'. This is followed by the candidates name as it appears in the voting ballot. There was no indication whether this was all the questions that were asked but there were tables provided by the pollster showing the breakdown by sex, education, preferred political affiliation and income. Therefore, it would be safe to assume that there were questions related to the socio-demographic characteristics of the respondents even though we don't know the specific questions asked. 

## Methodology: Population
The targeted population of this poll was not explicitly stated. While the pollster did conduct phone surveys, they did not mention any inclusion or exclusion criteria. Furthermore, the use of IVR during phone surveys and lack of information provided regarding the questions asked as discussed in @sec-patriot_polling_overview, it will be difficult to guarantee that the polls are targeting all eligible voters in Wisconsin. Given that nature of the polling article focuses on Wisconsin, it would be safe to assume that the targeted population would be eligible voters above 18 years old in Wisconsin.

## Methodology: Sampling Frame
Since phone surveys were conducted on both landlines and smartphones, the sampling frame of this poll would be eligible voters above 18 years old in Wisconsin with a working landline or possess a phone with a working contact number.     


## Strengths {#sec-WisconsinStrengths}
This automated method of phone surveys is a lot more convenient and efficient than in person type of surveys since it removes the need for a surveyor to visit respondents face to face and also removes the need for a surveyor to contact multiple respondents over the phone. This decreases labour effort and cost. 

Furthermore, with real-time feedback from phone surveys, the list of questions can be easily and quickly adjusted to ensure the ethical integrity of the survey being conducted. According to @patriotPollingMethodology, this survey took place in 2 days which could have taken a lot longer if other in-person type of surveying methods were used in this research. 

Particularly, the polling company used a repertoire of RDD sampling and stratified sampling on landlines to ensure that every landline block (strata) has equal numbers of respondents to be included in the final sample. This helps to ensure diversity from the sample by including different geographical landline blocks, reducing sampling bias. The purchasing of mobile phones from phone number databases abates sampling bias in areas where a particular demographic - younger people who are fully dependant on mobile phones and not landlines - would otherwise be underrepresented if only RDD sampling was conducted on landlines.  


## Limitations and weakness {#sec-WisconsinWeaknesses}
Firstly, there was a lack of information regarding how RDD was performed for the landline sample, how they handled non-working landlines, as well as the total sample size attributed from landline and mobile phone surveys. 

The handling of no-response entities were also not explicitly mentioned by Patriot Polling. This could translate to respondents who were chosen but did not pick up their mobile phones/landlines as it was a busy period in the day or due to potential heightened awareness of phone based scams. This means that the final sample obtained might not be representative of the Wisconsin voter population.

Also, the initial phrasing to the second question can very easily confuse respondents. Although the initial phrasing is followed by the senate candidate's name, the initial phrasing 'How will you vote for president' still uses the wrong word 'president' instead of 'senate'. This does not reflect a high quality survey, which could lead to decreased trust from the respondent to Patriot Polling, in turn potentially resulting in a premature end to the surveys, voiding the initial response.

Overall, the biggest weakness in the methodology would be the purchasing of phone numbers from a consumer database as it introduces selection bias. Individuals from middle to higher socio-economic status would have greater purchasing power than their lower income counterparts, which means individuals from lower socio-economic status may not have phones. In addition, elderlies within Wisconsin may not necessarily have mobile phones due to the lack of skills to use such technologies. Also, individuals would've opted for do-not-call registries, opted out of marketing databases or simply have prepaid phones which do not link to their personal information. This could mean an under representation of individuals from the said groups.

## Simulation exploring weaknesses in Patriot's Polling's methodology
To illustrate the biggest concern raised in @sec-WisconsinWeaknesses, we will be simulating a Patriot's Polling's methodology by using US Census data to obtain random our pool of samples that are representive of the population of Wisconsin with information about their age, race and county. Since the raw data provided by IPUM [@USACensusData] encoded states and counties, we combined the raw data with a supplementary excel file provided by IPUM to obtain the actual county name. We will be using the variable $CISMRTPHN$ from the census data which represents whether the particular respondent of the census data has a smartphone. This variable will be used to mirror a Patriot's Polling methodology of obtaining personal contact number by purchasing from an online database. We used this approach as only Wisconsin respondents with smartphones can possibly be included in a smartphone contact number database.

For the sake of discussion, we will be obtaining 2 samples. **Sample A will include respondents above the voting age of 18 in Wisconsin regardless of whether they possess a smartphone while sample B will only include respondents above the voting age of 18 in Wisconsin who have smartphones.** Sample B will mimic purchasing phone numbers from a database. 

@tbl-race_correct_representation shows us the proportion of white, black and asian races from sample A while @tbl-race_wrong_representation shows us that from sample B. We notice that black individuals from Wisconsin are underrepresented (8.7% vs 9.4%). According to @BlackPovertyWisconsin, black poverty rates in Wisconsin are 2.5 times higher than the overall Wisconsin poverty rate. This supports the finding that if we were to include only individuals with smartphones, certain racial groups might be underrepresented. Likewise, we are able to observe Asians being overrepresented (4.4% vs 3.8%). @AsianWealthUS exerts that for various educational attainment levels, Asians are earning more than individuals who belong to white, black or hispanic households. It is unsurprising that Asians are being overrepresented as higher wealth among the Asian population in Wisconsin would translate to higher purchasing power to purchase smartphones. Asians would naturally be more likely to be included in a sample where only smartphone users are considered, by extension, Asians would be more likely to be included in a sample obtained from a consumer database containing user contact numbers.
```{r}
#| echo: false
#| eval: true
#| label: tbl-race_correct_representation
#| tbl-cap: "proportion of races from non-smartphone-discriminating sample"
#| warning: false
census_data <- read_csv("../data/00-simulated_data/simulated_appendix_data.csv")

set.seed(304)
smartphone_only_sample <- census_data |>
  filter(has_smartphone == 1) |>
  sample_n(1000, replace=FALSE, weight = weight)

all_sampled <- census_data |>
  sample_n(1000, replace=FALSE, weight = weight)

race_table_full_sample <- all_sampled |> 
  group_by(race) |>
  count() |>
  ungroup() |>
  mutate(prop = paste0(round(n/sum(n) * 100,1),'%')) |>
  select(race, count = n, proportion = prop)

kable(race_table_full_sample)


```

```{r}
#| echo: false
#| eval: true
#| label: tbl-race_wrong_representation
#| tbl-cap: "proportion of races from smartphone-only sample"
#| warning: false

# asians overrepresented, blacks underrepresented
race_table_smartphone_sample <- smartphone_only_sample |> 
  group_by(race) |>
  count() |>
  ungroup() |>
  mutate(prop = paste0(round(n/sum(n) * 100,1),'%')) |>
  select(race, count = n, proportion_chosen = prop)


kable(race_table_smartphone_sample)
```

This becomes an issue because underrepresentation of black communities and overrepresentation of asian communities in Wisconsin would not reflect the actual voting preferences of a particular racial demographic, resulting in sampling bias, potentially misleading people who interpret a Patriot Polling's poll on Wisconsin or even their written articles.

Next, @tbl-age-differerences shows us the the percentage difference in individuals chosen categorised by age groups from sample A and B. We are immediately alerted to a 20% decrease in the number of elderlies chosen from sampling method B compared to sampling method A. As of 2024, elderlies above the age of 65 are either from the baby boomer generation or silent generation, who are known to experience difficulty when using smartphones. @SmartphoneByAge shows us that 97% of Americans between 18 to 49 have smartphones, 89% for that of age groups 50 to 64, and a lower 76% of the American elderlies above 65 year old who own smartphones. Therefore, by proceeding with sample B (parallel to purchasing phone numbers from consumer database) for which the inclusion critera selects individuals who own smartphones, we might be discounting the votes of elderly population, resulting in selection bias.

{{< pagebreak >}}

```{r}
#| echo: false
#| eval: true
#| label: tbl-age-differerences
#| tbl-cap: "percentage difference in individuals chosen from sample A and B by age groups"
#| warning: false

################
#      age 
#
# results: median age is lower, and more elderlies above 65 years old
#          are underrepresented when obtaining phone numbers straight from 
#          database
################ 
age_bin_count_full <- all_sampled |>
  count(age_bin)

age_table <- smartphone_only_sample |>
  count(age_bin) |>
  inner_join(age_bin_count_full,
             by = c('age_bin'='age_bin')) |>
  mutate(diff = n.x - n.y,
         perc_diff_1 = round(diff/n.y * 100, 0),
         perc_diff_2 = case_when(
           diff < 0 ~ paste0(perc_diff_1, '%'),
           .default = paste0('+', perc_diff_1, '%'))) |>
  arrange(desc(abs(perc_diff_1))) |>
  select(`age bin` = age_bin, 
         `sample A` = n.y,
         `sample B` = n.x, 
         `perc diff` = perc_diff_2)

kable(age_table)
```

{{< pagebreak >}}

# Idealised methodology for presidential election forecasting

## Context of presidential election
US General elections runs every 4 years. The current state of the US electoral system is a two-party system where the republican and democratic parties dominate the political space [@USPresidentialcontext]. During general elections, eligible voters in America are actually voting for a group of people called electors within their state. Elector candidates from 38 out of 50 states are usually pledged to support a specific political party. While there had been 'faithless electors' who cast a vote to the opposing party, they account for lesser than 1% of the elector population from the past 58 presidential elections. 'Faithless electors' had never affected the outcome of the elections [@FaithlessElectors]. If a elector candidate receives majority of votes by voters in their state, they gain the privilege of all electoral votes allocated for that state. All electors make up the electoral college. The electoral college would then casts their votes to their preferred presidential nominee, for which the majority would determine who becomes president. 

## Split electoral votes in Nebraska and Maine {#sec-NebraskaMaine}
In the states of Nebraska and Maine, individuals are voting not at the state level, but at the congressional district level. Nebraska has 3 districts while Maine has 2. One elector is chosen for each congressional district, and an additional 2 electors is given for the statewide majority. Unlike the other states which operate on a winner-takes-all, this difference in voting allows for greater variation in the outcome of the general elections [@Maine&NebraskaEdgeCase].   

## Idealised methodology: Pilot 
The purpose of a pilot testing is to ensure a well-designed election forecasting survey. Due to the nature of the general election characterised by the votes of the electoral college, random sampling has to be performed across all 50 states of America. \$5,000 should go towards a pilot test obtaining 200 respondents per state. This pilot test would target battleground states such as Wisconsin as well as Maine and Nebraska to test specific issues around a split vote between the two presidential candidate. Begin by using stratified sampling that divides each state population into strata by various socio demographic factors (e.g. gender, highest educational attainment, race, income), then conduct systematic random sampling within each strata. An optional \$5 digital or mailed voucher can be used to incentivise respondents to complete the survey.    

Respondents should be recruited quickly using by random digit dialing (RDD) to reduce selection bias. Firstly, obtain the list of possible area codes corresponding to the first 6 digits of a contact number within a state. Then randomly sample the remaining 4 digits. Coupled with the use of an automated interactive voice response system, this would increase efficiency while taking note of non-response from either premature end to phone surveys or non-working landlines/phonelines. This also decreases response bias as unlisted contact numbers would be included in the sampling frame as well. Theoretically, a larger than expected sample has to be recruited in order to account for contact numbers that are not in use. Also, it is important to distinguish landline RDD sampling and cellphone RDD sampling as they represent different communties within the population as mentioned in @sec-WisconsinStrengths and @sec-WisconsinWeaknesses

As for the contents of the phone survey, begin by providing detailed, succint information. We would introduce our company name, the purpose of the phone survey, and how only their polling preference is recorded and how their contact information would be used (but not saved) for the optional voucher redeeming purposes. If the phone survey is conducted in the first congressional district of Maine, explicitly mention that this survey is used to obtain polling preferences among voters within the first congressional district of Maine, and there are no political affiliations. It is paramount to obtain consent from the respondent. Therefore, have an option to let a respondent opt out of the survey without any form of discrimination. If a respondent chooses to continue the survey, we can ask them for their highest educational background, race, gender and/or other socio demographic information with the option to indicate non-response for any particular question. If a respondent successfully completes the phone survey, record the information into a secure, password protected excel sheet for further data analysis. It is also important to honor the incentives should the respondent complete the survey by using only the phone number. At the end, thank the respondent for their coorperative participation.

In order to expand the sampling frame to include voters who may not prefer to do a phone surveys, we can conduct a Google forms survey as well which is free of charge. The format of the form should mimic that of the RDD random sampling method. Refer to [this google form](https://forms.gle/aMUwiEs9yNwpT4tq8) for an example of the survey questions aksed.

After obtaining all pilot samples, validate the pilot data by checking for any patterns or outliers. In particular, we have to pay special attention to ensure that the samples received are balanced and representive of the voting population in that particular state by cross referencing to census data available from IPUMS US census data [@USACensusData]. If the data is not representive of a particular population, it might be necessary to conduct in person surveys in regions of the state where certain demographics are underrepresented.  
These forms of recruiting respondents should mimic the actual sampling conducted on a larger scale after a successful pilot.  

## Idealised methodology: Large scale sampling
By refining our phone surveys and google form surveys, we can expand the surveys to all states in America. For this we budget out \$80,000 to obtain between 2000 to 5000 samples across each state, depending on the state's population. For the states of Nebraska and Maine, we will be performing both analysis for individual congressional districts and statewide majority votes to determine the number of estimated electoral votes as described in @sec-NebraskaMaine. Particularly for all battleground states as seen from the past 2020 elections like Arizona, Florida and Maine's second congressional district, we will be oversampling to ensure that the preferences of sampled voters are better representive of the overall voter population in their particular state or congressional district, leading to reduced sampling error and more precise forecasting of the presidential elections. 

If the pilot discovers regions where certain demographics are underrepresented (e.g. poorer communities without digital communication devices nor landlines), surveyors would have to be employed in these regions to obtain samples directly.  

Ultimately, the algorithm to forecast the presidential winner involves obtaining the majority vote based on samples obtained within each state (and districts in Nebraska and Maine). Then based on the  number of electors assigned to that region, we will sum up the number of electors affiliated to the presidential candidates. This sum divided by the 538 total electors would be the predicted probability that a presidential candidate would win in the upcoming election. \$10,000 will be spent on the cost of surveyors. The remaining \$5,000 from our budget will be used as contigency funds.

## Presentation of polling results
We can map out the results to give a visual representation of how the votes mapped out to each state. @fig-hypotheticalvisualisation-1 provides a quick overview to an estimated presidential candidate preference in each state in America from the obtained samples. The colour distinction makes it quickly identifiable the majority within each state. @fig-hypotheticalvisualisation-2 provides the final predicted probability of how likely a party (presidential candidate) would be elected president. The following map utilises data obtained from Cook Political Report [@US2020mapdata]. Tigris library [@tigrisCitation] coupled with tidyverse library [@tidyverseReference] was used to generate the US map shown in @fig-hypotheticalvisualisation-1. Tidyverse library was also used to generate the bar chart shown in @fig-hypotheticalvisualisation-2. 

```{r map of US votes}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-hypotheticalvisualisation
#| fig-cap: "Forecast of US presidential elections"
#| fig-subcap: ["Map forecast", "Overall forecast"]
#| layout-ncol: 2


#####################################
# plot 1: map of forecasted elections
#
#####################################
library(tigris) # obtaining geographical data for USA
options(tigris_use_cache = TRUE)


votes_in_2020 <- read_csv('../data/01-raw_data/us_2020_votes.csv')

us_states <- states(cb = TRUE, resolution = '20m') |> #
  filter(NAME != 'Puerto Rico') |>
  shift_geometry() |>
  left_join(votes_in_2020, 
            by = c('NAME' = 'state'))

us_states |>
  ggplot(mapping = aes(fill = called)) + 
  geom_sf(color='white', lwd=0.5) +
  theme_void() +
  scale_fill_manual(
    values = c('D' = 'skyblue3', 'R' = 'indianred3'),
    labels = c('D'='Democrat','R'='Republican')) +
  labs(fill='Party',
       caption='Note: Maine and Nebraska are not operating under winner-takes-all',
       title='2024 US Presidential Election Forecast')

###########################
# plot 2: Bar chart
#
###########################
hypothetical_outcome <- tibble(
  'party' = c('Democrat', 'Republican'),
  'perc_electoral_votes' = c(51, 49)
)

label_y <- hypothetical_outcome$perc_electoral_votes / 2
text_label_y <- paste0(label_y, '%')

hypothetical_outcome |>
  ggplot(aes(x=party, y=perc_electoral_votes)) +
  geom_col(mapping = aes(fill = party), show.legend = FALSE) +
  geom_text(aes(y=label_y, 
                label=paste0(perc_electoral_votes, '%')), 
            colour = 'white', size=15) +
  theme(
    axis.text.x = element_text(size=15),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank()
  ) +
  labs(
    title = 'Forecasted probability for Republican and Democrats'
  ) +
  scale_fill_manual(
    values = c('skyblue3', 'indianred3'))
```


{{< pagebreak >}}

# Model details {#sec-model-details}

## Posterior predictive checks

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```



\newpage


# References


